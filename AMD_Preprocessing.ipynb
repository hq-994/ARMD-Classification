{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c3f195a-85c2-4c2e-a1d4-92bfc45ce581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Processing category: dryamd\n",
      "   ‚ûú Processing ABDUL_MAJEED_15-11-2012_P700255_(0009).jpg...\n",
      "   ‚ûú Processing Aisha_lasania_20-5-2008_P469051_(0001).jpg...\n",
      "   ‚ûú Processing BALGURI_BUCHAIAH_29-8-2009_N200166_(0000).jpg...\n",
      "   ‚ûú Processing BALGURI_BUCHAIAH_29-8-2009_N200166_(0004).jpg...\n",
      "   ‚ûú Processing BANDA_SATYAMMA_24-9-2012_P446897_(0011).jpg...\n",
      "   ‚ûú Processing BANDA_SATYAMMA_24-9-2012_P446897_(0033).jpg...\n",
      "   ‚ûú Processing BATHULA_CHITTAMMA_21-10-2009_P475278_(0009).jpg...\n",
      "   ‚ûú Processing BIPLABKUMAR_NANDY_21-9-2009_P545332_(0004).jpg...\n",
      "   ‚ûú Processing BIPLABKUMAR_NANDY_21-9-2009_P545332_(0011).jpg...\n",
      "   ‚ûú Processing BOPPANA_NAGESH WAR RAO_12-12-2009_P555348_(0016).jpg...\n",
      "   ‚ûú Processing CHINNA_RAJAMMA_22-11-2011_P648076_(0006).jpg...\n",
      "   ‚ûú Processing CHINNA_RAJAMMA_22-11-2011_P648076_(0012).jpg...\n",
      "   ‚ûú Processing DIPTI_B_3-2-2011_P589427_(0017).jpg...\n",
      "   ‚ûú Processing DURGADAS_MUNSHI_21-9-2009_P545175_(0006).jpg...\n",
      "   ‚ûú Processing DURGADAS_MUNSHI_21-9-2009_P545175_(0012).jpg...\n",
      "   ‚ûú Processing JAGADESHWAR_GANNU_5-11-2011_P645745_(0001).jpg...\n",
      "   ‚ûú Processing JOSEPH_AMMIRTHAMMAL_31-3-2011_P614034_(0003).jpg...\n",
      "   ‚ûú Processing KRISHNA KUMARI_B_14-7-2012_P663895_(0013).jpg...\n",
      "   ‚ûú Processing KRISHNA KUMARI_B_14-7-2012_P663895_(0027).jpg...\n",
      "   ‚ûú Processing KRISHNA REDDY_K.M_2-11-2012_P644453_(0000).jpg...\n",
      "   ‚ûú Processing KRISHNA REDDY_K.M_2-11-2012_P644453_(0017).jpg...\n",
      "   ‚ûú Processing masiuddin_bajid_1-1-2013_P693231_(0019).jpg...\n",
      "   ‚ûú Processing MOOL CHAND_DOSHI_7-4-2010_P569789_(0005).jpg...\n",
      "   ‚ûú Processing MRIDUL_KANTI GHOSH_28-10-2010_P590338_(0018).jpg...\n",
      "   ‚ûú Processing NEDUNURI_HARI ORASAD_22-9-2011_P640192_(0000).jpg...\n",
      "   ‚ûú Processing PONNAKKAMCHERI_NANI_31-1-2011_P529936_(0011).jpg...\n",
      "   ‚ûú Processing PONNAKKAMCHERI_NANI_31-1-2011_P529936_(0012).jpg...\n",
      "   ‚ûú Processing PRAMILA_SHAH_06-11-2012_P698801_(0011).jpg...\n",
      "   ‚ûú Processing PRAMILA_SHAH_06-11-2012_P698801_(0027).jpg...\n",
      "   ‚ûú Processing PREMLATHA_DALMIA_09-4-2010_P517026_(0001).jpg...\n",
      "   ‚ûú Processing PREMLATHA_DALMIA_09-4-2010_P517026_(0005).jpg...\n",
      "   ‚ûú Processing RACHAMALLA_ANASUYA_27-2-2012_P575291_(0006).jpg...\n",
      "   ‚ûú Processing ramulu_muraeishetty_04-6-2009_p531340_(0005).jpg...\n",
      "   ‚ûú Processing SACHINDRANATH_BAATTACHARYA_5-10-2012_P511735_(0015).jpg...\n",
      "   ‚ûú Processing SAMBA SHIVA RAO_PONDURI_30-1-2009_P515314_(0002).jpg...\n",
      "   ‚ûú Processing SHAMBHU KUMAR_SARKAR_13-9-2012_P630603_(0019).jpg...\n",
      "   ‚ûú Processing Shankar Lal_Mandhana_27-3-2012_P420664_(0021).jpg...\n",
      "   ‚ûú Processing SHANTAMMA_BANDI_10-6-2009_P532133_(0001).jpg...\n",
      "   ‚ûú Processing SHARMA_J N_11-1-2012_P592175_(0026).jpg...\n",
      "   ‚ûú Processing SHARMA_NEMI CHANDRA_27-9-2011_P640232_(0007).jpg...\n",
      "   ‚ûú Processing SHARMA_NEMI CHANDRA_27-9-2011_P640232_(0016).jpg...\n",
      "   ‚ûú Processing SHIVAJI RAO_SAHEB J_18-8-2011_P634531_(0000).jpg...\n",
      "   ‚ûú Processing SITA RAMACHANDRA RAJU_PENMETSA_3-1-2011_P565636_(0003).jpg...\n",
      "   ‚ûú Processing SITA RAMACHANDRA RAJU_PENMETSA_3-1-2011_P565636_(0009).jpg...\n",
      "   ‚ûú Processing SUBHAS_CHANDRA CHATTERJEE_1-9-2012_P689845_(0011).jpg...\n",
      "   ‚ûú Processing SUBHAS_CHANDRA CHATTERJEE_1-9-2012_P689845_(0026).jpg...\n",
      "   ‚ûú Processing SUSHEELA_MS_20-12-2008_P504500_(0000).jpg...\n",
      "   ‚ûú Processing SUSHEELA_MS_20-12-2008_P504500_(0004).jpg...\n",
      "   ‚ûú Processing UMA SHORI_MUNSHI_20-7-2009_P537481_(0011).jpg...\n",
      "   ‚ûú Processing ZACHERIAS_MARIADAS_05-10-2010_P592690_(0008).jpg...\n",
      "‚úÖ Finished processing dryamd!\n",
      "\n",
      "üìÇ Processing category: wetamd\n",
      "   ‚ûú Processing aarcot kannan_harigopal_15-7-2011_P489934_(0018).jpg...\n",
      "   ‚ûú Processing ABDUL_AZEEM_13-6-2012_P502226_(0000).jpg...\n",
      "   ‚ûú Processing ABDUL_MAJED_8-12-2010_P599177_(0009).jpg...\n",
      "   ‚ûú Processing ABDUL_MAJEED_15-11-2012_P700255_(0009).jpg...\n",
      "   ‚ûú Processing ABDUL_RAHEEMAN_11-5-2012_P672993_(0008).jpg...\n",
      "   ‚ûú Processing ADINARAYANA_D_9-2-2012_P659540_(0018).jpg...\n",
      "   ‚ûú Processing AHMED_MAJEED_21-7-2011_N199492_(0007).jpg...\n",
      "   ‚ûú Processing Aisha_lasania_20-5-2008_P469051_(0001).jpg...\n",
      "   ‚ûú Processing ALAM_NOOR_09-9-2011_P637300_(0008).jpg...\n",
      "   ‚ûú Processing ALLE_VENKATARAJAM_03-6-2009_P531249_(0006).jpg...\n",
      "   ‚ûú Processing ANAND_K ADYA_12-11-2009_P551720_(0012).jpg...\n",
      "   ‚ûú Processing ANUP_KUMAR_30-6-2011_P628202_(0001).jpg...\n",
      "   ‚ûú Processing ANUP_KUMAR_30-6-2011_P628202_(0014).jpg...\n",
      "   ‚ûú Processing APURBA KUMAR_GHOSH_15-6-2011_P625724_(0001).jpg...\n",
      "   ‚ûú Processing ARKALA_VIJAYA_21-9-2010_P591306_(0001).jpg...\n",
      "   ‚ûú Processing ASEEM_JALIUDDIN_04-7-2012_P681345_(0015).jpg...\n",
      "   ‚ûú Processing AVIJIT_BAIRAGI_4-4-2012_P667487_(0032).jpg...\n",
      "   ‚ûú Processing AVISHEK_SARKAR_5-12-2011_P649336_(0024).jpg...\n",
      "   ‚ûú Processing AYINAVALLI_BHASKAR RAO_18-6-2011_N231028_(0014).jpg...\n",
      "   ‚ûú Processing azeeza_begum_25-6-2009_p534034_(0000).jpg...\n",
      "   ‚ûú Processing azeeza_begum_25-6-2009_p534034_(0009).jpg...\n",
      "   ‚ûú Processing BALA_GOUD_08-6-2011_P624029_(0001).jpg...\n",
      "   ‚ûú Processing BALA_KISHAN_5-11-2012_P697903_(0026).jpg...\n",
      "   ‚ûú Processing BALA_LAXMI_31-3-2012_P569084_(0002).jpg...\n",
      "   ‚ûú Processing begum_naseem_23-10-2012_P697409_(0010).jpg...\n",
      "   ‚ûú Processing BHAGYALAXMI_S_28-02-2012_P661520_(0015).jpg...\n",
      "   ‚ûú Processing BHAGYALAXMI_S_28-02-2012_P661520_(0032).jpg...\n",
      "   ‚ûú Processing BIMAL CHANDRA_MUKHERJEE_23-6-2012_P679743_(0011).jpg...\n",
      "   ‚ûú Processing biswanath_mandal_7-8-2012_P686482_(0001).jpg...\n",
      "   ‚ûú Processing BITLA_LAXMAN RAJ_21-1-2010_P555952_(0001).jpg...\n",
      "   ‚ûú Processing BIVASH_RANJAN KAR_24-9-2011_P640447_(0001).jpg...\n",
      "   ‚ûú Processing BIVASH_RANJAN KAR_24-9-2011_P640447_(0018).jpg...\n",
      "   ‚ûú Processing Boya_Manemma_13-6-2011_N230481_(0003).jpg...\n",
      "   ‚ûú Processing CHALLA_MALLAIAH_06-02-2012_P603389_(0000).jpg...\n",
      "   ‚ûú Processing CHANDRAMOULI_A_20-6-2012_P678129_(0020).jpg...\n",
      "   ‚ûú Processing CHATRAPTHI_BASU_19-7-2011_P630953_(0017).jpg...\n",
      "   ‚ûú Processing chatterjee_sraboni_19-11-2011_P647763_(0066).jpg...\n",
      "   ‚ûú Processing CHINNAMMAI_LANKA_22-5-2009_P529762_(0011).jpg...\n",
      "   ‚ûú Processing CH_LAVANYA_10-1-2013_P708369_(0023).jpg...\n",
      "   ‚ûú Processing DAKSHINA_MURTHY_24-2-2012_P661743_(0003).jpg...\n",
      "   ‚ûú Processing DATTATRIYA_G AMBLE_10-2-2012_P659707_(0001).jpg...\n",
      "   ‚ûú Processing DILIP KUMAR_AGARWAL_23-2-2012_P661467_(0023).jpg...\n",
      "   ‚ûú Processing DR SINDHOORA_AMBATI_05-12-2011_P649945_(0001).jpg...\n",
      "   ‚ûú Processing FATHE_MOHAMMED_19-11-2009_P552591_(0005).jpg...\n",
      "   ‚ûú Processing garnedi_nagendramma_30-6-2009_P534972_(0003).jpg...\n",
      "   ‚ûú Processing garnedi_nagendramma_30-6-2009_P534972_(0010).jpg...\n",
      "   ‚ûú Processing GHOSH_MUKUL_9-9-2009_P519279_(0006).jpg...\n",
      "   ‚ûú Processing GITA_GHOSH_19-3-2011_P613373_(0014).jpg...\n",
      "   ‚ûú Processing GOSWAMI_RANJAN_20-3-2009_P509528_(0000).jpg...\n",
      "   ‚ûú Processing GUNTHA_PAVANI_4-8-2011_P633515_(0021).jpg...\n",
      "‚úÖ Finished processing wetamd!\n",
      "\n",
      "üìÇ Processing category: healthy\n",
      "   ‚ûú Processing 1ffa9627-8d87-11e8-9daf-6045cb817f5b..JPG...\n",
      "   ‚ûú Processing 1ffa9628-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa9629-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa962a-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa962b-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa962d-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa962e-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa962f-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa9630-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa9630-8d87-11e8-9daf-6045cb817f5b..JPG...\n",
      "   ‚ûú Processing 1ffa9631-8d87-11e8-9daf-6045cb817f5b..JPG...\n",
      "   ‚ûú Processing 1ffa9632-8d87-11e8-9daf-6045cb817f5b..JPG...\n",
      "   ‚ûú Processing 1ffa9633-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa9634-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa9638-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa9639-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa963a-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa963f-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa9640-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa9641-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa9642-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa9643-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa9644-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa9647-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa9649-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa964a-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa964b-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa964c-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa9651-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa9652-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa9654-8d87-11e8-9daf-6045cb817f5b. (1).jpeg...\n",
      "   ‚ûú Processing 1ffa9654-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa9655-8d87-11e8-9daf-6045cb817f5b. (1).jpeg...\n",
      "   ‚ûú Processing 1ffa9655-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa9656-8d87-11e8-9daf-6045cb817f5b. (1).jpeg...\n",
      "   ‚ûú Processing 1ffa9656-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa9657-8d87-11e8-9daf-6045cb817f5b. (1).jpeg...\n",
      "   ‚ûú Processing 1ffa9657-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa9658-8d87-11e8-9daf-6045cb817f5b. (1).jpeg...\n",
      "   ‚ûú Processing 1ffa9658-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa9659-8d87-11e8-9daf-6045cb817f5b. (1).jpeg...\n",
      "   ‚ûú Processing 1ffa9659-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa965a-8d87-11e8-9daf-6045cb817f5b. (1).jpeg...\n",
      "   ‚ûú Processing 1ffa965a-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "   ‚ûú Processing 1ffa965b-8d87-11e8-9daf-6045cb817f5b. (1).jpeg...\n",
      "   ‚ûú Processing 1ffa965b-8d87-11e8-9daf-6045cb817f5b..jpeg...\n",
      "‚úÖ Finished processing healthy!\n",
      "\n",
      "üéâ All processing complete! Check: C:\\Users\\sidda\\Downloads\\output_dir\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Define input and output base folders\n",
    "input_base_folder = r\"C:\\Users\\sidda\\Downloads\\amd1stattempt\"  # Change this to your dataset folder\n",
    "output_base_folder = r\"C:\\Users\\sidda\\Downloads\\output_dir\"\n",
    "\n",
    "# Define categories (subfolders inside input_base_folder)\n",
    "categories = [\"dryamd\", \"wetamd\", \"healthy\"]\n",
    "\n",
    "# Create output directories for each category\n",
    "for category in categories:\n",
    "    os.makedirs(os.path.join(output_base_folder, category), exist_ok=True)\n",
    "\n",
    "# CLAHE parameters (applied to the Green channel)\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "\n",
    "# Median filter kernel size\n",
    "kernel_size = 3  # Small kernel to avoid excessive blurring\n",
    "\n",
    "# Process images from each category folder\n",
    "for category in categories:\n",
    "    input_folder = os.path.join(input_base_folder, category)\n",
    "    output_folder = os.path.join(output_base_folder, category)\n",
    "\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"‚ö†Ô∏è Warning: {input_folder} does not exist!\")\n",
    "        continue\n",
    "\n",
    "    print(f\"üìÇ Processing category: {category}\")\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        if filename.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "            print(f\"   ‚ûú Processing {filename}...\")\n",
    "\n",
    "            # Read the color fundus image\n",
    "            img = cv2.imread(file_path)\n",
    "\n",
    "            if img is None:\n",
    "                print(f\"   ‚ö†Ô∏è Skipping {filename}: Could not read file.\")\n",
    "                continue\n",
    "\n",
    "            # Convert to LAB color space and apply CLAHE on L-channel\n",
    "            lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "            l, a, b = cv2.split(lab)\n",
    "            l_clahe = clahe.apply(l)\n",
    "            lab_clahe = cv2.merge((l_clahe, a, b))\n",
    "            img_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "            # Apply median filtering on CLAHE-enhanced image\n",
    "            filtered_img = cv2.medianBlur(img_clahe, kernel_size)\n",
    "\n",
    "            # Save the processed image\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            cv2.imwrite(output_path, filtered_img)\n",
    "\n",
    "    print(f\"‚úÖ Finished processing {category}!\\n\")\n",
    "\n",
    "print(\"üéâ All processing complete! Check:\", output_base_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5660ed0-57ad-4cec-af7a-8c8717b6f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.core.composition import OneOf\n",
    "from albumentations.augmentations import transforms\n",
    "\n",
    "# Define input and output folders\n",
    "input_base_folder = r\"C:\\Users\\sidda\\Downloads\\output_dir\"  # Folder with preprocessed images\n",
    "output_base_folder = \"augmented_images\"\n",
    "\n",
    "# Categories\n",
    "categories = [\"wet_amd\", \"dry_amd\", \"healthy\"]\n",
    "\n",
    "# Define augmentation pipeline\n",
    "augmentations = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=20, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "    A.GaussianBlur(p=0.3),\n",
    "    A.HueSaturationValue(p=0.3),\n",
    "])\n",
    "\n",
    "# Process each category\n",
    "for category in categories:\n",
    "    input_folder = os.path.join(input_base_folder, category)\n",
    "    output_folder = os.path.join(output_base_folder, category)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    image_files = [f for f in os.listdir(input_folder) if f.endswith(('.jpg', '.png'))]\n",
    "    num_original_images = len(image_files)\n",
    "    \n",
    "    print(f\"üìÇ Augmenting {category}: {num_original_images} images ‚ûù 150 images\")\n",
    "\n",
    "    # Generate augmented images\n",
    "    augmented_count = 0\n",
    "    while augmented_count < 150:  # We need 150 additional images\n",
    "        for img_name in image_files:\n",
    "            img_path = os.path.join(input_folder, img_name)\n",
    "            image = cv2.imread(img_path)\n",
    "\n",
    "            if image is None:\n",
    "                continue\n",
    "\n",
    "            # Apply augmentation\n",
    "            augmented = augmentations(image=image)[\"image\"]\n",
    "\n",
    "            # Save the augmented image\n",
    "            new_name = f\"{augmented_count+1}_{img_name}\"\n",
    "            cv2.imwrite(os.path.join(output_folder, new_name), augmented)\n",
    "\n",
    "            augmented_count += 1\n",
    "            if augmented_count >= 150:\n",
    "                break  # Stop when we reach 150 new images\n",
    "\n",
    "    print(f\"‚úÖ Finished augmenting {category}!\")\n",
    "\n",
    "print(\"üéâ Traditional augmentation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a8f9f9f-b5a9-4b6f-b3c6-85c55685111d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\sidda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.6.0)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.21.0-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.6.0-cp310-cp310-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\sidda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\sidda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\sidda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sidda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sidda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sidda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sidda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\sidda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sidda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sidda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached torchvision-0.21.0-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "Using cached torchaudio-2.6.0-cp310-cp310-win_amd64.whl (2.4 MB)\n",
      "Installing collected packages: torchvision, torchaudio\n",
      "Successfully installed torchaudio-2.6.0 torchvision-0.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85623825-15ce-4c7f-bc8e-3c13309380c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\sidda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.10.0)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pillow in c:\\users\\sidda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sidda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sidda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sidda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\sidda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\sidda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sidda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\sidda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sidda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sidda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install matplotlib pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e73bb072-4293-4ac8-82c2-c4aab70ce3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalizing between -1 and 1\n",
    "])\n",
    "\n",
    "# Dataset Paths\n",
    "train_path = r'C:\\Users\\sidda\\Downloads\\amdmypart\\augmented_images\\train'\n",
    "val_path = r'C:\\Users\\sidda\\Downloads\\amdmypart\\augmented_images\\val'\n",
    "\n",
    "# Load Datasets\n",
    "train_dataset = datasets.ImageFolder(train_path, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(val_path, transform=transform)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(\"‚úÖ Datasets loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47abf2fe-c2af-42fe-87dd-3ff525fbde1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Downsampling\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Bottleneck\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # ‚úÖ Upsampling with PixelShuffle\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.PixelShuffle(2),  # Replaces ConvTranspose2d\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 3, kernel_size=7, stride=1, padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00d4f6db-e914-4956-9c8d-788b64f1827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(128, 1, kernel_size=4, stride=1, padding=1),\n",
    "            nn.Sigmoid()  # Output between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788355c7-9f27-4f1b-825d-a8afbef95d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] - Generator Loss: 7.6599 - Discriminator Loss: 0.4643\n",
      "Epoch [2/50] - Generator Loss: 5.3198 - Discriminator Loss: 0.4280\n",
      "Epoch [3/50] - Generator Loss: 4.6248 - Discriminator Loss: 0.4011\n",
      "Epoch [4/50] - Generator Loss: 5.8197 - Discriminator Loss: 0.4868\n",
      "Epoch [5/50] - Generator Loss: 5.8968 - Discriminator Loss: 0.4235\n",
      "Epoch [6/50] - Generator Loss: 4.8991 - Discriminator Loss: 0.3339\n",
      "Epoch [7/50] - Generator Loss: 5.6078 - Discriminator Loss: 0.3221\n",
      "Epoch [8/50] - Generator Loss: 5.7166 - Discriminator Loss: 0.3723\n",
      "Epoch [9/50] - Generator Loss: 5.1347 - Discriminator Loss: 0.3495\n",
      "Epoch [10/50] - Generator Loss: 4.4628 - Discriminator Loss: 0.3989\n",
      "Epoch [11/50] - Generator Loss: 4.2441 - Discriminator Loss: 0.3884\n",
      "Epoch [12/50] - Generator Loss: 4.2819 - Discriminator Loss: 0.4335\n",
      "Epoch [13/50] - Generator Loss: 4.5533 - Discriminator Loss: 0.3956\n",
      "Epoch [14/50] - Generator Loss: 4.8878 - Discriminator Loss: 0.3790\n",
      "Epoch [15/50] - Generator Loss: 4.1725 - Discriminator Loss: 0.2972\n",
      "Epoch [16/50] - Generator Loss: 3.8853 - Discriminator Loss: 0.3237\n",
      "Epoch [17/50] - Generator Loss: 3.6570 - Discriminator Loss: 0.3301\n",
      "Epoch [18/50] - Generator Loss: 5.5938 - Discriminator Loss: 0.2619\n",
      "Epoch [19/50] - Generator Loss: 3.8376 - Discriminator Loss: 0.3279\n",
      "Epoch [20/50] - Generator Loss: 4.5157 - Discriminator Loss: 0.3904\n",
      "Epoch [21/50] - Generator Loss: 3.8588 - Discriminator Loss: 0.4199\n",
      "Epoch [22/50] - Generator Loss: 4.4712 - Discriminator Loss: 0.4379\n",
      "Epoch [23/50] - Generator Loss: 3.5123 - Discriminator Loss: 0.3917\n",
      "Epoch [24/50] - Generator Loss: 4.2031 - Discriminator Loss: 0.4387\n",
      "Epoch [25/50] - Generator Loss: 4.7829 - Discriminator Loss: 0.3965\n",
      "Epoch [26/50] - Generator Loss: 4.1649 - Discriminator Loss: 0.3299\n",
      "Epoch [27/50] - Generator Loss: 3.7362 - Discriminator Loss: 0.3760\n",
      "Epoch [28/50] - Generator Loss: 3.8447 - Discriminator Loss: 0.3796\n",
      "Epoch [29/50] - Generator Loss: 3.7453 - Discriminator Loss: 0.3949\n",
      "Epoch [30/50] - Generator Loss: 5.2592 - Discriminator Loss: 0.4878\n",
      "Epoch [31/50] - Generator Loss: 4.4113 - Discriminator Loss: 0.4323\n",
      "Epoch [32/50] - Generator Loss: 3.6004 - Discriminator Loss: 0.4075\n",
      "Epoch [33/50] - Generator Loss: 3.5692 - Discriminator Loss: 0.3924\n",
      "Epoch [34/50] - Generator Loss: 3.2277 - Discriminator Loss: 0.4206\n",
      "Epoch [35/50] - Generator Loss: 3.5062 - Discriminator Loss: 0.3857\n",
      "Epoch [36/50] - Generator Loss: 2.7567 - Discriminator Loss: 0.3561\n",
      "Epoch [37/50] - Generator Loss: 3.6115 - Discriminator Loss: 0.4598\n",
      "Epoch [38/50] - Generator Loss: 3.4736 - Discriminator Loss: 0.4276\n",
      "Epoch [39/50] - Generator Loss: 3.3309 - Discriminator Loss: 0.3501\n",
      "Epoch [40/50] - Generator Loss: 3.4172 - Discriminator Loss: 0.4360\n",
      "Epoch [41/50] - Generator Loss: 3.4373 - Discriminator Loss: 0.4521\n",
      "Epoch [42/50] - Generator Loss: 2.9529 - Discriminator Loss: 0.3913\n",
      "Epoch [43/50] - Generator Loss: 3.0394 - Discriminator Loss: 0.3489\n",
      "Epoch [44/50] - Generator Loss: 2.9576 - Discriminator Loss: 0.4005\n",
      "Epoch [45/50] - Generator Loss: 3.4962 - Discriminator Loss: 0.3881\n",
      "Epoch [46/50] - Generator Loss: 3.0059 - Discriminator Loss: 0.3832\n",
      "Epoch [47/50] - Generator Loss: 3.0706 - Discriminator Loss: 0.3866\n",
      "Epoch [48/50] - Generator Loss: 3.1105 - Discriminator Loss: 0.3939\n",
      "Epoch [49/50] - Generator Loss: 2.8415 - Discriminator Loss: 0.3679\n",
      "Epoch [50/50] - Generator Loss: 2.8246 - Discriminator Loss: 0.4019\n",
      "üéØ Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# Model Initialization\n",
    "G_AB = Generator().to(device)\n",
    "G_BA = Generator().to(device)\n",
    "D_A = Discriminator().to(device)\n",
    "D_B = Discriminator().to(device)\n",
    "\n",
    "# Loss Functions\n",
    "adversarial_loss = nn.MSELoss()\n",
    "cycle_consistency_loss = nn.L1Loss()\n",
    "\n",
    "# Optimizers\n",
    "lr = 0.0002\n",
    "beta1, beta2 = 0.5, 0.999\n",
    "g_optimizer = optim.Adam(list(G_AB.parameters()) + list(G_BA.parameters()), lr=lr, betas=(beta1, beta2))\n",
    "d_optimizer = optim.Adam(list(D_A.parameters()) + list(D_B.parameters()), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "# Training Parameters\n",
    "num_epochs = 50\n",
    "lambda_cycle = 20\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data_A, data_B) in enumerate(zip(train_loader, val_loader)):\n",
    "        real_A = data_A[0].to(device)\n",
    "        real_B = data_B[0].to(device)\n",
    "\n",
    "        # Adversarial Ground Truths (Dynamic Sizing)\n",
    "        valid = torch.ones_like(D_A(real_A), device=device)\n",
    "        fake = torch.zeros_like(D_A(real_A), device=device)\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "        g_optimizer.zero_grad()\n",
    "\n",
    "        # Identity Loss\n",
    "        loss_id_A = cycle_consistency_loss(G_BA(real_A), real_A)\n",
    "        loss_id_B = cycle_consistency_loss(G_AB(real_B), real_B)\n",
    "\n",
    "        # Adversarial Loss\n",
    "        fake_B = G_AB(real_A)\n",
    "        loss_gan_AB = adversarial_loss(D_B(fake_B), valid[:fake_B.size(0)])\n",
    "\n",
    "        fake_A = G_BA(real_B)\n",
    "        loss_gan_BA = adversarial_loss(D_A(fake_A), valid[:fake_A.size(0)])\n",
    "\n",
    "        # Cycle Consistency Loss\n",
    "        recovered_A = G_BA(fake_B)\n",
    "        loss_cycle_A = cycle_consistency_loss(recovered_A, real_A)\n",
    "\n",
    "        recovered_B = G_AB(fake_A)\n",
    "        loss_cycle_B = cycle_consistency_loss(recovered_B, real_B)\n",
    "\n",
    "        # Total Generator Loss\n",
    "        g_loss = (\n",
    "            loss_gan_AB + loss_gan_BA +\n",
    "            lambda_cycle * (loss_cycle_A + loss_cycle_B) +\n",
    "            0.5 * (loss_id_A + loss_id_B)\n",
    "        )\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminators\n",
    "        # ---------------------\n",
    "        d_optimizer.zero_grad()\n",
    "\n",
    "        # Real Loss\n",
    "        loss_real_A = adversarial_loss(D_A(real_A), valid[:real_A.size(0)])\n",
    "        loss_real_B = adversarial_loss(D_B(real_B), valid[:real_B.size(0)])\n",
    "\n",
    "        # Fake Loss\n",
    "        loss_fake_A = adversarial_loss(D_A(fake_A.detach()), fake[:fake_A.size(0)])\n",
    "        loss_fake_B = adversarial_loss(D_B(fake_B.detach()), fake[:fake_B.size(0)])\n",
    "\n",
    "        # Total Discriminator Loss\n",
    "        d_loss = (loss_real_A + loss_fake_A + loss_real_B + loss_fake_B) * 0.5\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Generator Loss: {g_loss.item():.4f} - Discriminator Loss: {d_loss.item():.4f}\")\n",
    "\n",
    "print(\"üéØ Training Complete!\")\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8128793a-117a-48fd-9abd-d2fdf452b02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 50 Images Generated for Each Category with Original Size!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# ‚úÖ Output Directories\n",
    "output_dirs = {\n",
    "    'wet_AMD': r'C:\\Users\\sidda\\Downloads\\amdmypart\\Results_of_gan\\wetamd',\n",
    "    'dry_AMD': r'C:\\Users\\sidda\\Downloads\\amdmypart\\Results_of_gan\\dryamd',\n",
    "    'healthy': r'C:\\Users\\sidda\\Downloads\\amdmypart\\Results_of_gan\\healthy'\n",
    "}\n",
    "\n",
    "# Automatically create directories if they don't exist\n",
    "for dir_path in output_dirs.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# ‚úÖ Function to Generate Images (Same Size as Input)\n",
    "def generate_images(generator, data_loader, output_dir, num_images=50):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    generator.eval()  # Set generator to evaluation mode\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in data_loader:\n",
    "            images = images.to(device)\n",
    "            fake_images = generator(images)\n",
    "\n",
    "            # Match the original size\n",
    "            if fake_images.size() != images.size():\n",
    "                fake_images = torch.nn.functional.interpolate(fake_images, size=(images.size(2), images.size(3)), mode='bilinear', align_corners=False)\n",
    "\n",
    "            for i in range(fake_images.size(0)):\n",
    "                if count >= num_images:\n",
    "                    return  # Stop after generating the required number of images\n",
    "                save_image(fake_images[i], os.path.join(output_dir, f\"generated_{count + 1}.png\"), normalize=True)\n",
    "                count += 1\n",
    "\n",
    "# ‚úÖ Generate 50 Images for Each Category\n",
    "generate_images(G_AB, train_loader, output_dirs['wet_AMD'], num_images=50)\n",
    "generate_images(G_BA, val_loader, output_dirs['dry_AMD'], num_images=50)\n",
    "generate_images(G_AB, val_loader, output_dirs['healthy'], num_images=50)\n",
    "\n",
    "print(\"‚úÖ 50 Images Generated for Each Category with Original Size!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7990c86a-f4ae-4809-aa0c-1113fd57f9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Resized Images Generated!\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Add Resize Transformation\n",
    "resize_transform = transforms.Resize((512, 512))  # Change (256, 256) to your desired size\n",
    "\n",
    "def generate_images(generator, data_loader, output_dir, num_images=50):\n",
    "    generator.eval()\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in data_loader:\n",
    "            images = images.to(device)\n",
    "            fake_images = generator(images)\n",
    "\n",
    "            for i in range(fake_images.size(0)):\n",
    "                if count >= num_images:\n",
    "                    return\n",
    "                # Apply the resizing transformation\n",
    "                resized_image = resize_transform(fake_images[i].cpu())\n",
    "                save_image(resized_image, os.path.join(output_dir, f\"generated_{count+1}.png\"), normalize=True)\n",
    "                count += 1\n",
    "\n",
    "# Generate resized images\n",
    "generate_images(G_AB, train_loader, output_dirs['wet_AMD'], num_images=50)\n",
    "generate_images(G_AB, val_loader, output_dirs['dry_AMD'], num_images=50)\n",
    "generate_images(G_AB, train_loader, output_dirs['healthy'], num_images=50)\n",
    "\n",
    "print(\"‚úÖ Resized Images Generated!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9fb569-9fe3-479a-ac9b-24448e1097b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660051e-bbe8-4e2d-a11f-cf33f6b18d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1ea4f3-ab79-48b4-92bd-ba50d74eb89b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa936d7-4aaf-4e9b-bb9d-34a7401ad42b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77abfa-070c-42ba-895d-fd11d5cac550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93b5a5d-92b4-463d-893f-f1a7b4b3a818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bdc399-1539-4ee1-8d4e-b627cddf4bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b117f064-af98-4e5f-8114-59801fdf5a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f17bb31-d8ef-4158-afb5-60da1c78d48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad523bd-60bb-4db1-9568-ab05bf57a33e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226d8cf1-faa9-4445-b386-f1c70b410858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913b8975-b877-4807-be0c-1b2ff0e7fbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e614e6-a143-433a-a58e-448a9b5eb630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b771e6e-e87d-4fd5-a231-f23776a0f479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d280d058-5c7f-4c71-9997-1d53d02c63e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39c6b5f-fed7-4a53-a81a-a8414f2c6414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ef0a7-6dcf-4cf3-aa23-3fd271991242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e113e-7619-4fd6-844e-8cb6726821f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabc9368-fced-4ec7-830f-4ed61aa970d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
