{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebc947d6-0cc1-4571-8825-3d358affe387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#r\"C:\\Users\\saifa\\OneDrive\\Desktop\\AMD Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc83597-5e9f-41bb-a471-77bcf0a6acf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (360, 1, 128, 128, 3) Labels: (360, 3)\n",
      "Validation data shape: (90, 1, 128, 128, 3) Labels: (90, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Parameters\n",
    "IMG_SIZE = (128, 128)  \n",
    "NUM_CLASSES = 3  # Wet AMD, Dry AMD, Healthy\n",
    "\n",
    "# Paths\n",
    "train_path = r\"C:\\Users\\saifa\\OneDrive\\Desktop\\train\"\n",
    "val_path = r\"C:\\Users\\saifa\\OneDrive\\Desktop\\val\"\n",
    "results_of_gan=r\"C:\\Users\\saifa\\OneDrive\\Desktop\\Results_of_gan\"\n",
    "\n",
    "class_folders = {\n",
    "    \"Wet_AMD\": \"wetamd\",\n",
    "    \"Dry_AMD\": \"dryamd\",\n",
    "    \"Healthy\": \"healthy\"\n",
    "}\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return None\n",
    "    img = cv2.resize(img, IMG_SIZE)\n",
    "    img = img / 255.0  # Normalize\n",
    "    return img\n",
    "\n",
    "# Load dataset\n",
    "def load_dataset(base_path):\n",
    "    X, y = [], []\n",
    "    for label, folder_name in enumerate(class_folders.values()):\n",
    "        folder_path = os.path.join(base_path, folder_name)\n",
    "        for file in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, file)\n",
    "            img = preprocess_image(img_path)\n",
    "            if img is not None:\n",
    "                X.append(img)\n",
    "                y.append(label)\n",
    "    return np.array(X), to_categorical(np.array(y), NUM_CLASSES)\n",
    "\n",
    "# Load training and validation data\n",
    "X_train, y_train = load_dataset(train_path)\n",
    "X_val, y_val = load_dataset(val_path)\n",
    "\n",
    "# Reshape for LSTM\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "X_val = X_val.reshape((X_val.shape[0], 1, IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape, \"Labels:\", y_train.shape)\n",
    "print(\"Validation data shape:\", X_val.shape, \"Labels:\", y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c900596c-817a-44e4-abed-5f14ecc445a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Training Data Size: (1440, 1, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data Augmentation\n",
    "data_gen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Augment Training Data\n",
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    img = X_train[i].reshape(IMG_SIZE[0], IMG_SIZE[1], 3)  # Remove time-distributed dimension\n",
    "    label = y_train[i]\n",
    "\n",
    "    img = img.reshape((1,) + img.shape)  # Convert to (1, height, width, channels)\n",
    "\n",
    "    for _ in range(3):  # Generate 3 augmented versions\n",
    "        augmented_img = next(data_gen.flow(img, batch_size=1))[0]  # Keep the 4D format\n",
    "        augmented_images.append(augmented_img)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train = np.array(augmented_images + list(X_train.reshape(-1, IMG_SIZE[0], IMG_SIZE[1], 3)))\n",
    "y_train = np.array(augmented_labels + list(y_train))\n",
    "\n",
    "# Restore LSTM-compatible shape (5D)\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "\n",
    "print(\"New Training Data Size:\", X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18e348c2-044c-4f64-9cb1-7a0a99a15c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved U-Net model loaded.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def improved_unet(input_size=(128, 128, 3)):\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    # Downsampling\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
    "\n",
    "    # Upsampling\n",
    "    u1 = UpSampling2D((2, 2))(c3)\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', padding='same')(u1)\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', padding='same')(c4)\n",
    "\n",
    "    u2 = UpSampling2D((2, 2))(c4)\n",
    "    c5 = Conv2D(64, (3, 3), activation='relu', padding='same')(u2)\n",
    "    c5 = Conv2D(64, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c5)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Compile & Train\n",
    "model = improved_unet()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Improved U-Net model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ce4b675-b21b-4ec0-aaef-0683381fdea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saifa\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved CNN-LSTM Model Loaded.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import TimeDistributed, Flatten, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "# Improved Hybrid CNN-LSTM Model\n",
    "model = Sequential([\n",
    "    TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'), input_shape=(1, IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2, 2))),\n",
    "    \n",
    "    TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2, 2))),\n",
    "\n",
    "    TimeDistributed(Flatten()),\n",
    "\n",
    "    LSTM(128, return_sequences=False),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "import tensorflow as tf  # Ensure TensorFlow is imported\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Improved CNN-LSTM Model Loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0389f568-af34-48c8-a5c9-3915314dc62f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 498ms/step - accuracy: 0.5378 - loss: 0.9122 - val_accuracy: 0.3333 - val_loss: 1.2095 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 473ms/step - accuracy: 0.7473 - loss: 0.6109 - val_accuracy: 0.3333 - val_loss: 1.3637 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 469ms/step - accuracy: 0.7311 - loss: 0.5424 - val_accuracy: 0.3333 - val_loss: 1.4329 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 467ms/step - accuracy: 0.7763 - loss: 0.4852 - val_accuracy: 0.3444 - val_loss: 1.3955 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 467ms/step - accuracy: 0.8020 - loss: 0.4058 - val_accuracy: 0.5222 - val_loss: 0.8603 - learning_rate: 5.0000e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 467ms/step - accuracy: 0.8234 - loss: 0.3957 - val_accuracy: 0.7556 - val_loss: 0.4280 - learning_rate: 5.0000e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 466ms/step - accuracy: 0.8608 - loss: 0.3447 - val_accuracy: 0.8333 - val_loss: 0.3593 - learning_rate: 5.0000e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 467ms/step - accuracy: 0.8844 - loss: 0.3096 - val_accuracy: 0.8667 - val_loss: 0.2872 - learning_rate: 5.0000e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 465ms/step - accuracy: 0.8996 - loss: 0.2753 - val_accuracy: 0.8667 - val_loss: 0.3042 - learning_rate: 5.0000e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 469ms/step - accuracy: 0.9152 - loss: 0.2585 - val_accuracy: 0.9000 - val_loss: 0.2611 - learning_rate: 5.0000e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 467ms/step - accuracy: 0.9251 - loss: 0.2235 - val_accuracy: 0.9000 - val_loss: 0.2544 - learning_rate: 5.0000e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 470ms/step - accuracy: 0.9544 - loss: 0.1891 - val_accuracy: 0.9222 - val_loss: 0.2280 - learning_rate: 5.0000e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 468ms/step - accuracy: 0.9497 - loss: 0.1635 - val_accuracy: 0.9444 - val_loss: 0.1986 - learning_rate: 5.0000e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 466ms/step - accuracy: 0.9768 - loss: 0.1332 - val_accuracy: 0.8778 - val_loss: 0.2091 - learning_rate: 5.0000e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 468ms/step - accuracy: 0.9809 - loss: 0.1038 - val_accuracy: 0.9444 - val_loss: 0.1940 - learning_rate: 5.0000e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 469ms/step - accuracy: 0.9854 - loss: 0.0876 - val_accuracy: 0.9444 - val_loss: 0.1707 - learning_rate: 5.0000e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 467ms/step - accuracy: 0.9858 - loss: 0.0850 - val_accuracy: 0.9222 - val_loss: 0.2069 - learning_rate: 5.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 469ms/step - accuracy: 0.9922 - loss: 0.0671 - val_accuracy: 0.9111 - val_loss: 0.1616 - learning_rate: 5.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 468ms/step - accuracy: 0.9955 - loss: 0.0520 - val_accuracy: 0.9111 - val_loss: 0.1872 - learning_rate: 5.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 465ms/step - accuracy: 0.9917 - loss: 0.0521 - val_accuracy: 0.9222 - val_loss: 0.1867 - learning_rate: 5.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 468ms/step - accuracy: 0.9964 - loss: 0.0399 - val_accuracy: 0.9333 - val_loss: 0.1615 - learning_rate: 5.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 468ms/step - accuracy: 0.9988 - loss: 0.0313 - val_accuracy: 0.9333 - val_loss: 0.1640 - learning_rate: 2.5000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 470ms/step - accuracy: 0.9995 - loss: 0.0282 - val_accuracy: 0.9444 - val_loss: 0.1687 - learning_rate: 2.5000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 468ms/step - accuracy: 0.9967 - loss: 0.0295 - val_accuracy: 0.9444 - val_loss: 0.1638 - learning_rate: 2.5000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 466ms/step - accuracy: 0.9976 - loss: 0.0270 - val_accuracy: 0.9444 - val_loss: 0.1738 - learning_rate: 1.2500e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 471ms/step - accuracy: 0.9993 - loss: 0.0205 - val_accuracy: 0.9444 - val_loss: 0.1699 - learning_rate: 1.2500e-05\n",
      "Training Completed!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "\n",
    "# Train\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_val, y_val),\n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "print(\"Training Completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afc3bf03-e51e-4a68-a8c2-9371cf8eca90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9198 - loss: 0.1779\n",
      "Model Accuracy: 93.33%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63d4fe02-e1a1-454a-8e72-d3b55cc28825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 659ms/step\n",
      "Predicted class: Healthy\n"
     ]
    }
   ],
   "source": [
    "def classify_image(image_path):\n",
    "    img = preprocess_image(image_path)\n",
    "    if img is None:\n",
    "        return \"Invalid image\"\n",
    "    img = img.reshape((1, 1, IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "    prediction = model.predict(img)\n",
    "    class_labels = ['Wet AMD', 'Dry AMD', 'Healthy']\n",
    "    return class_labels[np.argmax(prediction)]\n",
    "\n",
    "# Example usage\n",
    "image_path = r\"C:\\Users\\saifa\\OneDrive\\Desktop\\AMD Dataset\\Healthy Fundus\\1ffa965b-8d87-11e8-9daf-6045cb817f5b..jpeg\"\n",
    "result = classify_image(image_path)\n",
    "print(\"Predicted class:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf33ea32-b9ce-42cd-940b-71f4c74cc3cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 113ms/step\n",
      "Confusion Matrix:\n",
      "[[27  3  0]\n",
      " [ 3 27  0]\n",
      " [ 0  0 30]]\n",
      "\n",
      "Final Test Accuracy: 0.9333\n",
      "\n",
      "Metrics for Wet AMD:\n",
      "  Precision: 0.9000\n",
      "  Sensitivity (Recall): 0.9000\n",
      "  Specificity: 0.9500\n",
      "  Dice Coefficient: 0.9000\n",
      "\n",
      "Metrics for Dry AMD:\n",
      "  Precision: 0.9000\n",
      "  Sensitivity (Recall): 0.9000\n",
      "  Specificity: 0.9500\n",
      "  Dice Coefficient: 0.9000\n",
      "\n",
      "Metrics for Healthy:\n",
      "  Precision: 1.0000\n",
      "  Sensitivity (Recall): 1.0000\n",
      "  Specificity: 1.0000\n",
      "  Dice Coefficient: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Generate predictions on validation/test data\n",
    "y_pred = model.predict(X_val)  # Assuming X_val is your validation/test dataset\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels\n",
    "y_true_classes = np.argmax(y_val, axis=1)  # True class labels\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Compute final test accuracy\n",
    "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "print(f\"\\nFinal Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Compute per-class metrics\n",
    "precision = precision_score(y_true_classes, y_pred_classes, average=None)\n",
    "recall = recall_score(y_true_classes, y_pred_classes, average=None)\n",
    "f1 = f1_score(y_true_classes, y_pred_classes, average=None)\n",
    "\n",
    "# Sensitivity (Recall) and Specificity Calculation\n",
    "sensitivity = recall\n",
    "specificity = []\n",
    "for i in range(len(conf_matrix)):\n",
    "    tn = np.sum(conf_matrix) - (np.sum(conf_matrix[i, :]) + np.sum(conf_matrix[:, i]) - conf_matrix[i, i])\n",
    "    fp = np.sum(conf_matrix[:, i]) - conf_matrix[i, i]\n",
    "    specificity.append(tn / (tn + fp))\n",
    "\n",
    "# Dice coefficient calculation\n",
    "dice_coeff = (2 * precision * recall) / (precision + recall + 1e-7)  # Avoid division by zero\n",
    "\n",
    "# Print metrics for each class\n",
    "class_names = [\"Wet AMD\", \"Dry AMD\", \"Healthy\"]\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"\\nMetrics for {class_name}:\")\n",
    "    print(f\"  Precision: {precision[i]:.4f}\")\n",
    "    print(f\"  Sensitivity (Recall): {sensitivity[i]:.4f}\")\n",
    "    print(f\"  Specificity: {specificity[i]:.4f}\")\n",
    "    print(f\"  Dice Coefficient: {dice_coeff[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "623092b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 361ms/step - accuracy: 0.9935 - loss: 0.0414 - val_accuracy: 0.9222 - val_loss: 0.1645\n",
      "Epoch 2/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 360ms/step - accuracy: 0.9925 - loss: 0.0433 - val_accuracy: 0.9222 - val_loss: 0.1867\n",
      "Epoch 3/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 360ms/step - accuracy: 0.9939 - loss: 0.0408 - val_accuracy: 0.9444 - val_loss: 0.1776\n",
      "Epoch 4/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 358ms/step - accuracy: 0.9951 - loss: 0.0327 - val_accuracy: 0.9222 - val_loss: 0.1815\n",
      "Epoch 5/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 358ms/step - accuracy: 0.9958 - loss: 0.0269 - val_accuracy: 0.9444 - val_loss: 0.1771\n",
      "Epoch 6/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 353ms/step - accuracy: 0.9966 - loss: 0.0253 - val_accuracy: 0.9222 - val_loss: 0.1681\n",
      "Epoch 7/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 347ms/step - accuracy: 0.9964 - loss: 0.0243 - val_accuracy: 0.9222 - val_loss: 0.1575\n",
      "Epoch 8/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 349ms/step - accuracy: 0.9981 - loss: 0.0206 - val_accuracy: 0.9222 - val_loss: 0.1828\n",
      "Epoch 9/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 356ms/step - accuracy: 0.9990 - loss: 0.0162 - val_accuracy: 0.9444 - val_loss: 0.1752\n",
      "Epoch 10/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 351ms/step - accuracy: 0.9982 - loss: 0.0224 - val_accuracy: 0.9556 - val_loss: 0.1682\n",
      "Epoch 11/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 357ms/step - accuracy: 0.9993 - loss: 0.0155 - val_accuracy: 0.9222 - val_loss: 0.1621\n",
      "Epoch 12/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 360ms/step - accuracy: 0.9971 - loss: 0.0170 - val_accuracy: 0.9444 - val_loss: 0.1708\n",
      "Epoch 13/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 358ms/step - accuracy: 0.9990 - loss: 0.0149 - val_accuracy: 0.9444 - val_loss: 0.1692\n",
      "Epoch 14/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 357ms/step - accuracy: 0.9983 - loss: 0.0141 - val_accuracy: 0.9444 - val_loss: 0.1812\n",
      "Epoch 15/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 362ms/step - accuracy: 1.0000 - loss: 0.0136 - val_accuracy: 0.9556 - val_loss: 0.1932\n",
      "Epoch 16/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 358ms/step - accuracy: 0.9990 - loss: 0.0100 - val_accuracy: 0.9444 - val_loss: 0.2061\n",
      "Epoch 17/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 358ms/step - accuracy: 0.9994 - loss: 0.0108 - val_accuracy: 0.9222 - val_loss: 0.1792\n",
      "Epoch 18/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 359ms/step - accuracy: 0.9975 - loss: 0.0102 - val_accuracy: 0.9333 - val_loss: 0.1988\n",
      "Epoch 19/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 357ms/step - accuracy: 1.0000 - loss: 0.0108 - val_accuracy: 0.9222 - val_loss: 0.1854\n",
      "Epoch 20/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 356ms/step - accuracy: 0.9978 - loss: 0.0115 - val_accuracy: 0.9556 - val_loss: 0.1775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U-Net model saved as unet_model.h5\n"
     ]
    }
   ],
   "source": [
    "# Train U-Net model\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Assuming X_train and Y_train contain segmented images and masks\n",
    "model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val))\n",
    "\n",
    "# Save the trained U-Net model\n",
    "model.save(\"unet_model.h5\")\n",
    "print(\"U-Net model saved as unet_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9525008",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 360ms/step - accuracy: 0.9984 - loss: 0.0116 - val_accuracy: 0.9444 - val_loss: 0.1769\n",
      "Epoch 2/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 357ms/step - accuracy: 0.9987 - loss: 0.0070 - val_accuracy: 0.9333 - val_loss: 0.2209\n",
      "Epoch 3/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 358ms/step - accuracy: 0.9991 - loss: 0.0074 - val_accuracy: 0.9556 - val_loss: 0.1968\n",
      "Epoch 4/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 356ms/step - accuracy: 0.9999 - loss: 0.0061 - val_accuracy: 0.9222 - val_loss: 0.2423\n",
      "Epoch 5/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 359ms/step - accuracy: 0.9990 - loss: 0.0086 - val_accuracy: 0.9222 - val_loss: 0.2313\n",
      "Epoch 6/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 359ms/step - accuracy: 1.0000 - loss: 0.0059 - val_accuracy: 0.9222 - val_loss: 0.2067\n",
      "Epoch 7/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 359ms/step - accuracy: 0.9989 - loss: 0.0075 - val_accuracy: 0.9222 - val_loss: 0.2334\n",
      "Epoch 8/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 358ms/step - accuracy: 0.9979 - loss: 0.0069 - val_accuracy: 0.9222 - val_loss: 0.2417\n",
      "Epoch 9/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 359ms/step - accuracy: 0.9989 - loss: 0.0054 - val_accuracy: 0.9556 - val_loss: 0.2183\n",
      "Epoch 10/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 359ms/step - accuracy: 0.9983 - loss: 0.0065 - val_accuracy: 0.9444 - val_loss: 0.2373\n",
      "Epoch 11/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 360ms/step - accuracy: 0.9998 - loss: 0.0038 - val_accuracy: 0.9222 - val_loss: 0.2709\n",
      "Epoch 12/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 359ms/step - accuracy: 0.9999 - loss: 0.0054 - val_accuracy: 0.9111 - val_loss: 0.2377\n",
      "Epoch 13/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 359ms/step - accuracy: 0.9994 - loss: 0.0040 - val_accuracy: 0.9000 - val_loss: 0.2509\n",
      "Epoch 14/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 360ms/step - accuracy: 0.9996 - loss: 0.0044 - val_accuracy: 0.9000 - val_loss: 0.2663\n",
      "Epoch 15/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 361ms/step - accuracy: 0.9991 - loss: 0.0045 - val_accuracy: 0.9000 - val_loss: 0.2868\n",
      "Epoch 16/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 360ms/step - accuracy: 0.9989 - loss: 0.0065 - val_accuracy: 0.9000 - val_loss: 0.2608\n",
      "Epoch 17/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 359ms/step - accuracy: 0.9999 - loss: 0.0039 - val_accuracy: 0.9000 - val_loss: 0.2744\n",
      "Epoch 18/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 359ms/step - accuracy: 0.9998 - loss: 0.0030 - val_accuracy: 0.9333 - val_loss: 0.2066\n",
      "Epoch 19/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 359ms/step - accuracy: 0.9988 - loss: 0.0085 - val_accuracy: 0.9222 - val_loss: 0.2173\n",
      "Epoch 20/20\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 361ms/step - accuracy: 0.9976 - loss: 0.0068 - val_accuracy: 0.9111 - val_loss: 0.2419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid CNN-LSTM model saved as hybrid_model.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train Hybrid CNN-LSTM model\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val))\n",
    "\n",
    "# Save the trained CNN-LSTM model\n",
    "model.save(\"hybrid_model.h5\")\n",
    "print(\"Hybrid CNN-LSTM model saved as hybrid_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0fbb473-f538-4136-b3c5-4ba888f93d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # After training\n",
    "# model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# model.save(\"model.h5\")  # Ensures compilation info is saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fec9c8b-c17d-4cf0-899c-b543ddd7c9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing WetAMD: 100%|████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 268.09it/s]\n",
      "Processing DryAMD: 100%|████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 242.90it/s]\n",
      "Processing Healthy: 100%|███████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 250.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Metrics saved to unet_evaluation_report.csv\n",
      "\n",
      "--- Average Metrics Per Class ---\n",
      "         Edge Strength  Entropy  Mean Intensity  Foreground Ratio\n",
      "Class                                                            \n",
      "DryAMD          0.0273   4.5834          0.3215            0.1432\n",
      "Healthy         0.0371   4.5436          0.3431            0.1973\n",
      "WetAMD          0.0280   4.6013          0.3168            0.1447\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.filters import sobel\n",
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Metric Functions ===\n",
    "def edge_strength(image):\n",
    "    return np.mean(sobel(image))\n",
    "\n",
    "def image_entropy(image):\n",
    "    histogram, _ = np.histogram(image.flatten(), bins=256, range=(0, 1), density=True)\n",
    "    return entropy(histogram + 1e-10)  # Avoid log(0)\n",
    "\n",
    "def mean_intensity(image):\n",
    "    return np.mean(image)\n",
    "\n",
    "def foreground_ratio(image, threshold=0.5):\n",
    "    return np.sum(image > threshold) / image.size\n",
    "\n",
    "# === Process Folder Function ===\n",
    "def evaluate_folder(folder_path, class_name):\n",
    "    results = []\n",
    "    for filename in tqdm(os.listdir(folder_path), desc=f\"Processing {class_name}\"):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tif')):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            # Read and normalize image\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img = img / 255.0  # Normalize to [0,1]\n",
    "\n",
    "            # Compute metrics\n",
    "            result = {\n",
    "                \"Class\": class_name,\n",
    "                \"Image Name\": filename,\n",
    "                \"Edge Strength\": edge_strength(img),\n",
    "                \"Entropy\": image_entropy(img),\n",
    "                \"Mean Intensity\": mean_intensity(img),\n",
    "                \"Foreground Ratio\": foreground_ratio(img)\n",
    "            }\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "# === Main Evaluation ===\n",
    "def evaluate_all_classes(wet_path, dry_path, healthy_path, output_csv='unet_evaluation_report.csv'):\n",
    "    all_results = []\n",
    "\n",
    "    all_results += evaluate_folder(wet_path, \"WetAMD\")\n",
    "    all_results += evaluate_folder(dry_path, \"DryAMD\")\n",
    "    all_results += evaluate_folder(healthy_path, \"Healthy\")\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"\\n✅ Metrics saved to {output_csv}\")\n",
    "\n",
    "    # Print class-wise averages\n",
    "    print(\"\\n--- Average Metrics Per Class ---\")\n",
    "    class_avg = df.groupby(\"Class\").mean(numeric_only=True)\n",
    "    print(class_avg.round(4))\n",
    "\n",
    "# === USAGE ===\n",
    "# Replace these with your actual paths\n",
    "wet_path     = r\"C:\\Users\\saifa\\OneDrive\\Desktop\\seg_imgs_colour\\wetamd\"\n",
    "dry_path     = r\"C:\\Users\\saifa\\OneDrive\\Desktop\\seg_imgs_colour\\dryamd\"\n",
    "healthy_path = r\"C:\\Users\\saifa\\OneDrive\\Desktop\\seg_imgs_colour\\healthy\"\n",
    "\n",
    "evaluate_all_classes(wet_path, dry_path, healthy_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "971d8fe7-f64e-4d7b-ae3f-ef7646659273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* Running on public URL: https://eb123f75b5825309c5.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://eb123f75b5825309c5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed image shape: (1, 1, 128, 128, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step\n",
      "Processed image shape: (1, 1, 128, 128, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Processed image shape: (1, 1, 128, 128, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "Processed image shape: (1, 1, 128, 128, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(\"unet_model.h5\")\n",
    "\n",
    "# Define Labels (Modify as per your model)\n",
    "labels = [\"Wet AMD\", \"Healthy Fundus\", \"Dry AMD\"]  # Replace with actual class names\n",
    "\n",
    "# Function to Process Input and Predict\n",
    "def predict(img):\n",
    "    img = img.convert(\"RGB\")  # Ensure image is in RGB format\n",
    "    img = img.resize((128, 128))  # Resize to match model input size\n",
    "    img_array = np.array(img, dtype=np.float32) / 255.0  # Normalize\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add time-step dimension\n",
    "\n",
    "    print(\"Processed image shape:\", img_array.shape)  # Debugging\n",
    "\n",
    "    # Make prediction\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = labels[np.argmax(predictions)]\n",
    "    confidence = np.max(predictions) * 100\n",
    "    return f\"\\n🧠 *Prediction:* {predicted_class}\\n\"# 🎯 *Confidence:* {confidence:.2f}% \n",
    "   \n",
    "\n",
    "# Create Gradio Interface\n",
    "iface = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=gr.Image(type=\"pil\", label=\"📤 Upload an Image\"),\n",
    "    outputs=gr.Textbox(label=\"📊 Prediction Results\"),\n",
    "    title=\"🔍 Hybrid Model Prediction\",\n",
    "    theme=\"soft\",\n",
    ")\n",
    "\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61f94808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load model\n",
    "model = load_model(\"model.h5\")\n",
    "\n",
    "# Define class labels (modify if needed)\n",
    "labels = [\"Wet AMD\", \"Healthy Fundus\", \"Dry AMD\"]\n",
    "\n",
    "def predict_from_path(img_path):\n",
    "    # Load and preprocess image\n",
    "    img = Image.open(img_path).convert(\"RGB\")  # Ensure RGB\n",
    "    img = img.resize((128, 128))  # Resize to match model input\n",
    "    img_array = np.array(img, dtype=np.float32) / 255.0  # Normalize\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add time-step dimension (1, 128, 128, 3)\n",
    "\n",
    "    print(\"Processed image shape:\", img_array.shape)  # Debugging\n",
    "\n",
    "    # Make prediction\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = labels[np.argmax(predictions)]\n",
    "    confidence = np.max(predictions) * 100\n",
    "    return f\"🧠 *Prediction:* {predicted_class}\\n🎯 *Confidence:* {confidence:.2f}%\"\n",
    "\n",
    "# Example usage\n",
    "# image_path = r\"C:\\Users\\saifa\\OneDrive\\Desktop\\AMD Dataset\\Healthy Fundus\\1ffa962b-8d87-11e8-9daf-6045cb817f5b..jpeg\"  # Replace with actual file path\n",
    "# result = predict_from_path(image_path)\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a069b61c-a206-4977-91ca-9dbc82772bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://d87113bc5ad91bb240.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d87113bc5ad91bb240.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained CNN-LSTM model\n",
    "model = load_model(\"model.h5\")  # Make sure this is the correct final hybrid model path\n",
    "\n",
    "# Class labels (make sure they match model training order)\n",
    "labels = [\"Wet AMD\", \"Dry AMD\", \"Healthy\"]\n",
    "\n",
    "# Define image input size\n",
    "IMG_SIZE = (128, 128)\n",
    "\n",
    "def predict(img: Image.Image):\n",
    "    # Preprocess the uploaded image\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = img.resize(IMG_SIZE)\n",
    "    img_array = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    # Reshape to match CNN-LSTM input shape: (batch, time_step, height, width, channels)\n",
    "    img_array = img_array.reshape(1, 1, IMG_SIZE[0], IMG_SIZE[1], 3)\n",
    "\n",
    "    # Predict\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = labels[np.argmax(predictions)]\n",
    "    confidence = np.max(predictions) * 100\n",
    "\n",
    "    # Return result\n",
    "    return f\"🧠 *Prediction:* {predicted_class}\\n🎯 *Confidence:* {confidence:.2f}%\"\n",
    "\n",
    "# Gradio UI setup\n",
    "iface = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=gr.Image(type=\"pil\", label=\"📤 Upload Fundus Image\"),\n",
    "    outputs=gr.Textbox(label=\"📊 Prediction\"),\n",
    "    title=\"🔍 AMD Classifier | CNN-LSTM Model\",\n",
    "    description=\"Upload a fundus image to classify it as Wet AMD, Dry AMD, or Healthy Retina.\",\n",
    "    theme=\"soft\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cafbcd7c-26b9-49b2-8a24-a9f76afe665b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* Running on public URL: https://a7ab30e3555b978926.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a7ab30e3555b978926.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# —— Load your trained CNN-LSTM model —— \n",
    "model = load_model(\"model.h5\")\n",
    "\n",
    "# —— Make sure this matches your training order! —— \n",
    "labels = [\"Wet AMD\", \"Dry AMD\", \"Healthy\"]\n",
    "\n",
    "IMG_SIZE = (128, 128)\n",
    "\n",
    "def classify_fn(img: Image.Image):\n",
    "    # Preprocess\n",
    "    img = img.convert(\"RGB\").resize(IMG_SIZE)\n",
    "    arr = np.array(img, dtype=np.float32) / 255.0\n",
    "    arr = arr.reshape(1, 1, IMG_SIZE[0], IMG_SIZE[1], 3)\n",
    "\n",
    "    # Predict\n",
    "    preds = model.predict(arr)\n",
    "    idx = np.argmax(preds)\n",
    "    conf = float(np.max(preds) * 100)\n",
    "\n",
    "    return f\"🧠 **Prediction:** {labels[idx]}\\n🎯 **Confidence:** {conf:.2f}%\"\n",
    "\n",
    "# —— Build the Gradio Blocks interface —— \n",
    "with gr.Blocks(theme=\"soft\") as demo:\n",
    "    # Title + description\n",
    "    gr.Markdown(\"## 🔎 AMD Classifier | CNN-LSTM Model\")\n",
    "    gr.Markdown(\"Upload a fundus image to classify it as Wet AMD, Dry AMD, or Healthy Retina.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # Left column: upload + clear\n",
    "        with gr.Column(scale=1):\n",
    "            inp = gr.Image(type=\"pil\", label=\"📤 Upload Fundus Image\", elem_id=\"upload-box\")\n",
    "            clr = gr.Button(\"Clear\", elem_id=\"clear-btn\")\n",
    "        # Right column: prediction + flag\n",
    "        with gr.Column(scale=1):\n",
    "            out = gr.Textbox(label=\"📊 Prediction\", interactive=False, elem_id=\"prediction-box\")\n",
    "            flag = gr.Button(\"Flag\", elem_id=\"flag-btn\")\n",
    "\n",
    "    # Submit below both columns\n",
    "    submit = gr.Button(\"Submit\", variant=\"primary\", elem_id=\"submit-btn\")\n",
    "\n",
    "    # —— Wiring up events —— \n",
    "    submit.click(fn=classify_fn, inputs=inp, outputs=out)\n",
    "    clr.click(lambda: None, None, inp)  # clears the image input\n",
    "\n",
    "# Launch with shareable link\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69711c0f-8d82-4eb1-9060-62bf66566bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
